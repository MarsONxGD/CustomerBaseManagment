import json
import logging
import os
import re
import sys
from pathlib import Path

import pandas as pd
import torch
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk.tokenize import word_tokenize
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
)

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

from src.email_classifier.model import EmailClassifier

stem = True
if stem:
    stemmer = SnowballStemmer("russian")
    stop_words = set(stopwords.words("russian"))


def setup_logging():
    logging.basicConfig(level=logging.INFO)
    return logging.getLogger(__name__)


logger = setup_logging()


def load_model_and_vocab(model_path, vocab_path):
    with open(vocab_path, "r", encoding="utf-8") as f:
        vocab = json.load(f)

    checkpoint = torch.load(model_path, map_location=torch.device("cpu"))

    model = EmailClassifier(
        vocab_size=checkpoint["vocab_size"],
        embedding_dim=checkpoint["embedding_dim"],
        hidden_dim=checkpoint["hidden_dim"],
        output_dim=checkpoint["output_dim"],
        n_layers=checkpoint["n_layers"],
        dropout=checkpoint["dropout"],
        bidirectional=checkpoint["bidirectional"],
    )

    model.load_state_dict(checkpoint["model_state_dict"])
    model.eval()

    return model, vocab


def preprocess_text(text, vocab, max_length=200):
    """–¢–æ—á–Ω–∞—è –∫–æ–ø–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑ predict.py"""
    try:
        text = text.lower()
        text = re.sub(r"\S+@\S+", "", text)
        text = re.sub(r"http\S+", "", text)
        text = re.sub(r"[^–∞-—è–ê-–Ø—ë–Å\s]", " ", text)
        text = re.sub(r"\s+", " ", text).strip()

        if stem:
            pre_tokens = word_tokenize(text, language="russian")
            tokens = [
                stemmer.stem(token)
                for token in pre_tokens
                if token not in stop_words and len(token) > 2
            ]
        else:
            tokens = text.split()

        if len(tokens) < 1:
            logger.warning(f"–¢–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(tokens)}")
            return None, None

        indexed = [vocab.get(token, vocab["<UNK>"]) for token in tokens]
        text_length = min(len(indexed), max_length)

        if len(indexed) > max_length:
            indexed = indexed[:max_length]
        else:
            indexed = indexed + [vocab["<PAD>"]] * (max_length - len(indexed))

        return (
            torch.tensor(indexed, dtype=torch.long).unsqueeze(0),
            torch.tensor(text_length, dtype=torch.long).unsqueeze(0),
        )

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}")
        return None, None


def calculate_metrics():
    model_path = PROJECT_ROOT / "models" / "email_classifier.pth"
    vocab_path = PROJECT_ROOT / "models" / "vocabulary.json"
    test_data_path = PROJECT_ROOT / "datasets" / "test_data.csv"

    model, vocab = load_model_and_vocab(model_path, vocab_path)
    print("‚úÖ –ú–æ–¥–µ–ª—å –∏ —Å–ª–æ–≤–∞—Ä—å –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

    try:
        df = pd.read_csv(test_data_path)
        texts = df["text"].tolist()
        true_labels = df["label"].tolist()
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")

        class_counts = df['label'].value_counts()
        print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
        print(f"   –ö–ª–∞—Å—Å 0 (–ù–µ –∑–∞—è–≤–∫–∞): {class_counts.get(0, 0)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   –ö–ª–∞—Å—Å 1 (–ó–∞—è–≤–∫–∞): {class_counts.get(1, 0)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    except FileNotFoundError:
        print(f"‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª {test_data_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return

    predicted_labels = []
    predicted_probs = []
    processed_count = 0
    failed_count = 0

    print("\nüîÆ –í—ã–ø–æ–ª–Ω—è—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")

    with torch.no_grad():
        for i, (text, true_label) in enumerate(zip(texts, true_labels)):
            input_tensor, length_tensor = preprocess_text(text, vocab)

            if input_tensor is None:
                predicted_labels.append(0)
                predicted_probs.append(0.0)
                failed_count += 1
                continue

            try:
                output = model(input_tensor, length_tensor)
                probabilities = torch.softmax(output, dim=1)
                predicted_class = torch.argmax(output, dim=1).item()
                confidence = torch.max(probabilities).item()

                predicted_labels.append(predicted_class)
                predicted_probs.append(probabilities[0][1].item())
                processed_count += 1

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ {i}: {e}")
                predicted_labels.append(0)
                predicted_probs.append(0.0)
                failed_count += 1
                continue

            if (i + 1) % 10 == 0:
                print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(texts)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    print(f"üìà –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {processed_count} —É—Å–ø–µ—à–Ω–æ, {failed_count} —Å –æ—à–∏–±–∫–∞–º–∏")

    pred_class_0 = predicted_labels.count(0)
    pred_class_1 = predicted_labels.count(1)

    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –∫–ª–∞—Å—Å 0 (–ù–µ –∑–∞—è–≤–∫–∞): {pred_class_0}")
    print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –∫–ª–∞—Å—Å 1 (–ó–∞—è–≤–∫–∞): {pred_class_1}")

    if pred_class_0 == 0 or pred_class_1 == 0:
        print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å!")
        print("   –≠—Ç–æ –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å –æ–±—É—á–µ–Ω–∏–µ–º –∏–ª–∏ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é –¥–∞–Ω–Ω—ã—Ö")

    accuracy = accuracy_score(true_labels, predicted_labels)
    precision = precision_score(true_labels, predicted_labels, zero_division=0)
    recall = recall_score(true_labels, predicted_labels, zero_division=0)
    f1 = f1_score(true_labels, predicted_labels, zero_division=0)

    print("\n" + "=" * 60)
    print("üìä –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò")
    print("=" * 60)
    print(f"üéØ Accuracy (–¢–æ—á–Ω–æ—Å—Ç—å):  {accuracy:.4f} ({accuracy * 100:.2f}%)")
    print(f"üìè Precision (–¢–æ—á–Ω–æ—Å—Ç—å): {precision:.4f}")
    print(f"üìà Recall (–ü–æ–ª–Ω–æ—Ç–∞):     {recall:.4f}")
    print(f"‚öñÔ∏è  F1-Score:            {f1:.4f}")

    cm = confusion_matrix(true_labels, predicted_labels)
    print("\n" + "=" * 60)
    print("üîÑ –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö")
    print("=" * 60)
    print("\t\t\t–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ")
    print("\t\t\t–ù–µ—Ç\t\t–î–∞")
    print(f"–†–µ–∞–ª—å–Ω–æ\t–ù–µ—Ç\t{cm[0, 0]:3d}\t\t{cm[0, 1]:3d}")
    print(f"\t\t–î–∞\t{cm[1, 0]:3d}\t\t{cm[1, 1]:3d}")

    print("\n" + "=" * 60)
    print("üìã –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢")
    print("=" * 60)
    print(
        classification_report(
            true_labels,
            predicted_labels,
            target_names=["–ù–µ –∑–∞—è–≤–∫–∞", "–ó–∞—è–≤–∫–∞"],
            zero_division=0
        )
    )

    print("\n" + "=" * 60)
    print("üéØ –ê–ù–ê–õ–ò–ó –£–í–ï–†–ï–ù–ù–û–°–¢–ò –ú–û–î–ï–õ–ò")
    print("=" * 60)

    confidence_class_0 = [prob for pred, prob in zip(predicted_labels, predicted_probs) if pred == 0]
    confidence_class_1 = [prob for pred, prob in zip(predicted_labels, predicted_probs) if pred == 1]

    if confidence_class_0:
        print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –∫–ª–∞—Å—Å–∞ 0 (–ù–µ –∑–∞—è–≤–∫–∞):")
        print(f"   –°—Ä–µ–¥–Ω—è—è: {sum(confidence_class_0) / len(confidence_class_0):.3f}")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è: {min(confidence_class_0):.3f}")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è: {max(confidence_class_0):.3f}")

    if confidence_class_1:
        print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –∫–ª–∞—Å—Å–∞ 1 (–ó–∞—è–≤–∫–∞):")
        print(f"   –°—Ä–µ–¥–Ω—è—è: {sum(confidence_class_1) / len(confidence_class_1):.3f}")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è: {min(confidence_class_1):.3f}")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è: {max(confidence_class_1):.3f}")

    print("\n" + "=" * 60)
    print("üîç –ü–†–ò–ú–ï–†–´ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô")
    print("=" * 60)

    correct_count = 0
    incorrect_examples = []

    for i, (true, pred, prob, text) in enumerate(
            zip(true_labels, predicted_labels, predicted_probs, texts)
    ):
        status = "‚úÖ" if true == pred else "‚ùå"
        if true == pred:
            correct_count += 1
        else:
            incorrect_examples.append((true, pred, prob, text))

        if i < 3:
            class_name_true = "–ó–∞—è–≤–∫–∞" if true == 1 else "–ù–µ –∑–∞—è–≤–∫–∞"
            class_name_pred = "–ó–∞—è–≤–∫–∞" if pred == 1 else "–ù–µ –∑–∞—è–≤–∫–∞"
            confidence = prob if pred == 1 else (1 - prob)

            print(f"{status} –ü—Ä–∏–º–µ—Ä {i + 1}:")
            print(f"   –ò—Å—Ç–∏–Ω–∞: {class_name_true}")
            print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: {class_name_pred}")
            print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f}")
            print(f"   –¢–µ–∫—Å—Ç: {text[:80]}...")
            print()

    if incorrect_examples:
        print(f"\n‚ùå –û–®–ò–ë–û–ß–ù–´–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø (–ø–µ—Ä–≤—ã–µ 3):")
        for i, (true, pred, prob, text) in enumerate(incorrect_examples[:3]):
            class_name_true = "–ó–∞—è–≤–∫–∞" if true == 1 else "–ù–µ –∑–∞—è–≤–∫–∞"
            class_name_pred = "–ó–∞—è–≤–∫–∞" if pred == 1 else "–ù–µ –∑–∞—è–≤–∫–∞"
            confidence = prob if pred == 1 else (1 - prob)

            print(f"   –û—à–∏–±–∫–∞ {i + 1}:")
            print(f"      –ò—Å—Ç–∏–Ω–∞: {class_name_true}, –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: {class_name_pred}")
            print(f"      –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f}")
            print(f"      –¢–µ–∫—Å—Ç: {text[:60]}...")

    print(f"\nüìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {correct_count}/{len(texts)} ({correct_count / len(texts) * 100:.1f}%)")
    print(f"   –û—à–∏–±–æ–∫: {len(incorrect_examples)}/{len(texts)} ({len(incorrect_examples) / len(texts) * 100:.1f}%)")

    if pred_class_0 == 0 or pred_class_1 == 0:
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        print(f"   1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
        print(f"   2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö")
        print(f"   3. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
        print(f"   4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞")

if __name__ == "__main__":
    calculate_metrics()